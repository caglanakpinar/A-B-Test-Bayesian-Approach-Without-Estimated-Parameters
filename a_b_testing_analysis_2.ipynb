{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2, chi2_contingency\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more information pls check gitHub reporsitory: https://github.com/caglanakpinar/a_b_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_start = '2019-08-05' # this the day that my sample ab_tesitng data sset is started.\n",
    "is_from_db = False # Normally, this notebook runs from my local postgre sql db. \n",
    "# However, I also attached the .csv file of each data frame and zip them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In order to run sample data sets, make sure unzip ab.test_data.csv\n",
    "# This data is randomly created. It is not related to any companies or web sites. For more infırmation about how randomly it creates, you may check a_b_test_data_creator.ipnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Defination\n",
    "- I want to make a improvement on my web page. I wonder that how this development effect my clients attributes.\n",
    "#### Main Question; is the any difference between each screen of attractions?\n",
    "- In order to answer this question I will sample from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I need?\n",
    "- I need binary transaction for each client of visits. It must be well splited into Control and Validation Sets.\n",
    "- Control and Validation sets of sample size can be equal if it is possible.\n",
    "- In orer to sample, for each variation I use RAndom Sampling. \n",
    "- However, I segmented clients list, in each cluster of my client data set, randomly I picked my data sets.\n",
    "- In order to find the right sample size for each segment. This calculation will worked well;\n",
    "### segment_sample_size_control =  (segment_client_count/ total_client_count) * total_control_sample_size\n",
    "- Segmentation is another problem, but I have already segmented my clients by RFM segmentation.\n",
    "- I am going to calculate for whole sample of Control and Validation Sets, also, I will calculate my CTR values for each RFM segmets too.\n",
    "- After I have sampled my data, I will calculate the Click Through Rate for each metrics which are login, basket, login_screen, ordered for Control and Validation samples.\n",
    "- CTR_login = login_count / session_count\n",
    "- CTR_basket = basket_count / session_count\n",
    "- CTR_basket = order_screen_count / session_count\n",
    "- CTR_basket = order_count / session_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data set has clients of transactions.\n",
    "- Sample data set has two variation of a web site. It also has the clients of attributes after they see two variation.\n",
    "#### Metrics of check are; \n",
    "##### login: \n",
    "- It shows client sign in after seeing the screen in their visits. (0: Not Signing in (Not click) , 1: Signing in (click))\n",
    "##### basket: \n",
    "- It shows adding any product to thier basket in visits after they sign in (0: Didn`t add anything in basket, 1: There is a basket (click)) \n",
    "##### order_screen: \n",
    "- It shows being at payment screen after they add any products to thier baskets in their visits. (0: didn`t visit payment screen, 1: visited payment screen (click))\n",
    "##### order_count:\n",
    "- It shows ordering action of each transaction of client after their visits. (0: not ordered, 1: ordered (click))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_from_db:\n",
    "    query = \"\"\"\n",
    "            SELECT\n",
    "                client_id,\n",
    "                is_control,\n",
    "                sum(CASE WHEN session IS NOT NULL THEN 1 ELSE 0 END) as session_count,\n",
    "                sum(CASE WHEN login IS NOT NULL THEN 1 ELSE 0 END) as login_count,\n",
    "                sum(CASE WHEN basket IS NOT NULL THEN 1 ELSE 0 END) as basket_count,\n",
    "                sum(CASE WHEN order_screen IS NOT NULL THEN 1 ELSE 0 END) as order_screen_count,\n",
    "                sum(CASE WHEN ordered IS NOT NULL THEN 1 ELSE 0 END) as order_count\n",
    "            \n",
    "            FROM designingtest WHERE date = '{}'\n",
    "            GROUP BY client_id, is_control\n",
    "    \"\"\".format(str(date_start)[0:10])\n",
    "    ab_test = pd.read_sql(query, connection_abtestdb)    \n",
    "else:\n",
    "    ab_test = pd.read_csv(\"ab_test_start_date.csv\")  \n",
    "ab_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_from_db:\n",
    "    query = \"\"\"\n",
    "            SELECT frequency_segment, monetary_segment, recency_segment, client_id\n",
    "            FROM client_segments_daily WHERE date = '{}'\n",
    "    \"\"\".format(str(datetime.datetime.strptime(date_start, '%Y-%m-%d') + datetime.timedelta(days=2))[0:10])\n",
    "    segments = pd.read_sql(query, connection_clientsdb)\n",
    "else:\n",
    "    segments = pd.read_csv(\"segments.csv\") \n",
    "display(segments.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ab_test = pd.merge(ab_test, segments, on='client_id', how='left')\n",
    "ab_test['rfm'] = ab_test.apply(lambda row: \n",
    "                               str(row['recency_segment']) + '_' + \n",
    "                               str(row['frequency_segment']) + '_' + str(row['monetary_segment']) ,axis=1)\n",
    "ab_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('list of RFM segments :')\n",
    "list(ab_test['rfm'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A / B Test - Frequentist Approach\n",
    "## Two Sample t- test:\n",
    "- t Test allows us to answer the main question 'is the any difference between each screens of attractions?'\n",
    "- Two sample t-test of confidence interval will show us how accuratelly new variation of screen worked. Two sample are Control VS Validation\n",
    "- I will decided to implement my test with significance level of % 95.\n",
    "- By accepting significance level of % 95, we also assume that %5 of error by H0 hyphothesis.\n",
    "- In t - test we will work, We assume that CTR values will have t-distribution with estimated parameters X_mean and Standart Deviations of each control and Validation Sample.\n",
    "- CTR ~ t(X_mean, Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated Parameters \n",
    "- In roder to calculate t value for t-test, we need estimated parameters. \n",
    "- Beacuse we don`t know the population we are working on the sample of it which we assume that it represents it.\n",
    "#### In this problem CTR values are paremters for t- distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let`s calcualte parameters (CTR values fo each metrics for each segments);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will calculate CTR values for each segment and client.\n",
    "# Later, in order to calculate main data sets of CTR values, calculating the avreage of them will help me to find the CTR_main.\n",
    "ab_test_pivot = ab_test.pivot_table(index=['rfm', 'is_control'], aggfunc={'session_count':'sum',\n",
    "                                                                    'login_count':'sum',\n",
    "                                                                   'basket_count':'sum',\n",
    "                                                                   'order_screen_count':'sum',\n",
    "                                                                   'order_count':'sum'}).reset_index()\n",
    "# CTR_login = login_ratio\n",
    "ab_test_pivot['login_ratio'] = ab_test_pivot['login_count'] / ab_test_pivot['session_count']\n",
    "# CTR_basket = basket_ratio\n",
    "ab_test_pivot['basket_ratio'] = ab_test_pivot['basket_count'] / ab_test_pivot['session_count']\n",
    "# CTR_order_screen = order_screen_ratio\n",
    "ab_test_pivot['order_screen_ratio'] = ab_test_pivot['order_screen_count'] / ab_test_pivot['session_count']\n",
    "# CTR_order = order_ratio\n",
    "ab_test_pivot['order_ratio'] = ab_test_pivot['order_count'] / ab_test_pivot['session_count']\n",
    "ab_test_pivot = ab_test_pivot.reset_index()\n",
    "ab_test_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let`s see how Control and Validation set of CTR values are distributedon each metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in ['login_ratio', 'basket_ratio', 'order_screen_ratio', 'order_ratio']:\n",
    "    print(i, \" Check:\")\n",
    "    x1 = ab_test_pivot[ab_test_pivot['is_control'] == 0][i].values\n",
    "    y1 = ab_test_pivot[ab_test_pivot['is_control'] == 0]['index'].values\n",
    "    x2 = ab_test_pivot[ab_test_pivot['is_control'] == 1][i].values\n",
    "    y2 = ab_test_pivot[ab_test_pivot['is_control'] == 1]['index'].values\n",
    "    print('CTR (Parameters for t - distribution) Preditor of Control and Validation Group :')\n",
    "    print(\"CTR_Control_\" + i + \" :\", round(np.mean(x1), 2), \" || \", \"CTR_Validation_\" + i + \" :\", round(np.mean(x2), 2))\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    ax1.scatter(x1, y1, s=10, c='b', marker=\"s\", label='control')\n",
    "    ax1.scatter(x2,y2, s=10, c='r', marker=\"o\", label='validation')\n",
    "    plt.legend(loc='upper left');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let`s calculate the t - tests of each metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H0: There is no difference on control and validation test sets.\n",
    "### H1: Thre statistically difference between Control And Validation sets of click ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is calculation of t value for t- distribution\n",
    "def calculate_t_test(p1, p2, click1, click2, n1, n2):\n",
    "    p = (click1 + click2) / (n1 + n2)\n",
    "    t = (p1 - p2) / sqrt(( p * (1 - p)) / ((1 / n1) + (1 / n2)) )\n",
    "    df = n1 + n2 - 2\n",
    "    pval = 1 - stats.t.sf(np.abs(t), df)*2  # two-sided pvalue = Prob(abs(t)>tt)\n",
    "    confidence_limit = 1.96 * sqrt(((p1 * (1 - p1)) / n1) + ((p2 * (1 - p2)) / n2))\n",
    "    confidence_intervals = [abs(p1 - p2) - confidence_limit, abs(p1 - p2) + confidence_limit]\n",
    "    return pval, confidence_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = ['login_count', 'basket_count', 'order_screen_count', 'order_count']\n",
    "n1 = sum(list(ab_test_pivot[ab_test_pivot['is_control'] == 0]['session_count']))\n",
    "n2 = sum(list(ab_test_pivot[ab_test_pivot['is_control'] == 1]['session_count']))\n",
    "\n",
    "for metric in metrics:\n",
    "    print(\"----- \", i, \" Check :\", \"-----\")\n",
    "    print(\"H0 : There is no difference on control and validation test sets on \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" = \", \"p_validation_\", metric)\n",
    "    print(\"H1: Thre statistically difference between Control And Validation sets \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" != \", \"p_validation_\", metric)\n",
    "    print()\n",
    "    click1 =  sum(list(ab_test_pivot[ab_test_pivot['is_control'] == 1][metric]))\n",
    "    click2 =  sum(list(ab_test_pivot[ab_test_pivot['is_control'] == 0][metric]))\n",
    "    print(\"Click Control :\",  click1, \" || \\n Click Validation :\",  click2, \n",
    "          \" || \\n Sample Size Control :\", n1, \" || \\n Sample Size Validation :\", n2)\n",
    "    print()\n",
    "    pval, confidence_intervals = calculate_t_test(click1 / n1, click2 / n2, click1, click2, n1, n2)\n",
    "    print(\"p - value :\", round(1- pval, 2), \" || \\n\",\n",
    "          \"Control_CTR_\" + metric + \" :\", round(click1 / n1, 2), \" || \\n\",\n",
    "          \"Validation_CTR_\" + metric + \" :\", round(click2 / n2, 2), \" || \\n\",\n",
    "          \"-- HO REJECTED! --\" if 1 - pval > 0.975 or 1 - pval < 0.25 else \"-- HO ACCEPTED! --\")\n",
    "    print(\"left - right tail :\", confidence_intervals)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "- Assumption of H0 is rejeceted for each metrics. There is significant difference between Control and Validation CTR statistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T - Test Result Of Confidence;\n",
    "- T test is a parametric test that needs estimeted paramters of data. In this problem our paramter is CTR which represents p ratios.\n",
    "- However, t - test gives us a very general results of similarities between Control and Validation CTR`s, It is better to test the diffrence and similarities between Control and VAlidation set without using CTR parameter.\n",
    "- In this case, It is  better to use one of the non-parametric test which is Chi_Square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let`s see with Chi-Squared Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t test is a test which runs with parameter of p proportion. \n",
    "- However, most of the time we don`t know the exact estimated p proportion.\n",
    "- That is why, it is better to test also with Non Parametrics Test.\n",
    "- Proportion of distributions are Chi-Squared. \n",
    "- This test is also one tail test. It is also convient to our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to valvulate Chi-Squared value for Chi-Square distribution,\n",
    "# I need click and non click count of control and Validation sets.\n",
    "def chi_square_test(total_control, click_control, total_validation, click_validation):\n",
    "    observed_df = pd.DataFrame([\n",
    "                    {'test': 'control', 'click': click_control, 'non_click': total_control - click_control},\n",
    "                    {'test': 'validation', 'click': click_validation, 'non_click': total_control - click_validation},\n",
    "                    {'test': 'total', 'click': click_control + click_validation, \n",
    "                     'non_click': (total_control - click_control) + (total_control - click_validation)},\n",
    "                  ])\n",
    "    observed_df['total'] = observed_df['click'] + observed_df['non_click']\n",
    "    \n",
    "    total = list(observed_df[observed_df['test'] == 'total']['total'])[0]\n",
    "    \n",
    "    x2_val = 0\n",
    "    for g in ['control', 'validation']:\n",
    "        for v in ['click', 'non_click']:\n",
    "            _expected = (list(observed_df[observed_df['test'] == g]['total'])[0] * \n",
    "                         list(observed_df[observed_df['test'] == 'total'][v])[0]) / total\n",
    "            _observed = list(observed_df[observed_df['test'] == g][v])[0]\n",
    "            \n",
    "            #print(_expected, _observed, pow((_observed - _expected), 2) / _expected)\n",
    "            x2_val += pow((_observed - _expected), 2) / _expected\n",
    "    pval = chi2.cdf(x=x2_val, df=1)\n",
    "    print(pval, \"HO REJECTED!\" if pval > 0.95 else \"HO ACCEPTED!\")\n",
    "    return [pval, \"HO REJECTED!\" if pval > 0.95 else \"HO ACCEPTED!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let`s run the each metric of Chi - Squared Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = ['login_count', 'basket_count', 'order_screen_count', 'order_count']\n",
    "n1 = sum(list(ab_test[ab_test['is_control'] == 0]['session_count']))\n",
    "n2 = sum(list(ab_test[ab_test['is_control'] == 1]['session_count']))\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric, \" Check:\")\n",
    "    print(\"H0 : There is no difference on control and validation test sets on \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" = \", \"p_validation_\", metric)\n",
    "    print(\"H1: Thre statistically difference between Control And Validation sets \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" != \", \"p_validation_\", metric)\n",
    "    click1 =  sum(list(ab_test[ab_test['is_control'] == 0][metric]))\n",
    "    click2 =  sum(list(ab_test[ab_test['is_control'] == 1][metric]))\n",
    "    \n",
    "    print(click1, click2, n1, n2)\n",
    "    outputs = chi_square_test(n1, click1, n2, click2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of Chi - Square:\n",
    "- Same as t- test, it is obvious that sample Control is significantly different from sample Validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist Approach disadvantes;\n",
    "- However we test are differences between two sample, we are still not sure about the estimater parameter value. \n",
    "- In t - test we assume that CTR values of each samples represents population CTR values well enough to test We also assume that if make mistake about that assumption it will be less than %5.\n",
    "- CTR value which is parameter of t - distribution, is an unknown parameter of population Mean statistic. It is estimator. We don`t know exact mean. We only use estimated of it.\n",
    "- On the other hand we try Chi-Squared non-parametric test in order to get ride of estimated parameter value.\n",
    "- In this case we also test the difference of two sample, without knowledge of CTR values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let`s check for each metrics with Bayesian Process. \n",
    "### Why we do this?\n",
    "- It is because each ratios for metric is of parameters not well known. We have the data, but we don`t know the parameters.\n",
    "- Bayesian Process allows us to finding the ratios of distribution without using the parameter.\n",
    "- It deals with clicks data as P(X|Q) distribution X ~ Bernolli Because it only has (0, 1) which are click or non-click. (x : transaction of each metric ex: login_count_ x = [1,1,1, 0, 1, ... , 1,0,0])\n",
    "- The knowledge of the parameter Q probability of P(Q) which is a probability distribution. Values of X are between [0, 1] That is why it will have ßeta Distibution with parameter of a and b.\n",
    "- P(X|Q); X ~ Bernolli and P(Q); ßeta(a, b) are priors. \n",
    "- Bayes Theorem:  P(Q|X) = P(X|Q) * P(Q). This means when we have ratio of knowledge of parameter(P(Q)) and probability of X data given P(X|Q), I can calculate  parameter with this priors.\n",
    "- Bernolli (X ~ Bernolli) * Beta(Q ~ ßeta) Distributions also shown Beta(a - 1 + ∑X, b - 1 + ∑X) which is posterior.\n",
    "- So, if we check beta pdf of values by using a = \"click count\" and b = \" non click count\", at the end we might get the parameter without assuming as it`s prediction as like we did at t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimated CTR for login count :\" ,\n",
    "      round(np.mean(list(ab_test_pivot[ab_test_pivot['is_control'] == 0]['login_ratio'])),2),\n",
    "      round(np.mean(list(ab_test_pivot[ab_test_pivot['is_control'] == 1]['login_ratio'])),2))\n",
    "p_value_login_control = list(ab_test[ab_test['is_control'] == 0]['login_count'])\n",
    "p_value_login_validation = list(ab_test[ab_test['is_control'] == 1]['login_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = ['login_count', 'basket_count', 'order_screen_count', 'order_count']\n",
    "control = ab_test[ab_test['is_control'] == 0]\n",
    "validation = ab_test[ab_test['is_control'] == 1]\n",
    "\n",
    "def plot_p_value(x, y1, y2, p1, p2):\n",
    "    plt.plot(x, y1, label=\"real p: %.4f\" % p1)\n",
    "    plt.plot(x, y2, label=\"real p: %.4f\" % p2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# check each metrics \n",
    "for metric in metrics:\n",
    "    print(metric, \" Check:\")\n",
    "    print(\"Is Validation Set of \", metric, \"Conversion Rate is bigger than Control set?\")\n",
    "\n",
    "    x = np.linspace(0, 1, 200) # this is for ploting the results\n",
    "    a_control, b_control = 1, 1 # at first we started we can assign beta a, b shape parameters defult 1 and 1.\n",
    "    a_val, b_val = 1, 1 # it must be two beta distribution that is why I assing for each validaiton and control set of a and b.\n",
    "    c_val, v_val = list(control[metric]), list(validation[metric])\n",
    "    for ind in list(range(max(len(control), len(validation)))):\n",
    "        # control set a, b updating\n",
    "        # for each click update a and b paramters. a = click, b = non click\n",
    "        try:\n",
    "            a_control += c_val[ind] # click\n",
    "            b_control += abs(c_val[ind] - 1) # non-click\n",
    "        except:\n",
    "            if ind + 1 == len(control):\n",
    "                print(\"out of index\")\n",
    "        \n",
    "        # validation set a, b updating\n",
    "        try:\n",
    "            a_val += v_val[ind] # click\n",
    "            b_val += abs(v_val[ind] - 1) # non-click\n",
    "        except:\n",
    "            if ind + 1 == len(validation):\n",
    "                print(\"out of index\")\n",
    "                \n",
    "    \n",
    "        if ind in [10, 50, 100, 1000]: # check for given indexes to make sure it is updating CTR parameter.\n",
    "            print(\"number of sample :\", ind)\n",
    "            y_control = beta.pdf(x, a_control, b_control)  # beta probabilty density function for control a, b parameter\n",
    "            y_validation = beta.pdf(x, a_val, b_val) # beta probabilty density function for  validation a, b parameter\n",
    "            plot_p_value(x, y_control, y_validation, a_control / (a_control + b_control), a_val / (a_val + b_val))\n",
    "            \n",
    "    # check how many times vaidation CTR wins, control CTR\n",
    "    control_p_values = stats.beta.rvs(a_control, b_control, size=len(control))\n",
    "    validation_p_values = stats.beta.rvs(a_val, b_val, size=len(control))      \n",
    "    wins = validation_p_values > control_p_values\n",
    "    # ratio of wins: It also represents how accuratelly Validation Set has bigger CTR than Control Set.  \n",
    "    print(\"percent of times that Validation wins : \", np.mean(wins) * 100) \n",
    "\n",
    "\n",
    "    \n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result of Bayesian Approach:\n",
    "- It is better with unkown parameter at the end it finalize with estimated parameter with Beta Distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let`s run whole A/B Test for each days and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows me to calculate the sample sizes, clieck count of each metrics\n",
    "def get_metrics(df, metric, day, rfm):\n",
    "    print(\"H0 : There is no difference on control and validation test sets on \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" = \", \"p_validation_\", metric)\n",
    "    print(\"H1: Thre statistically difference between Control And Validation sets \", metric, \" ratios.\")\n",
    "    print(\"p_control_\", metric, \" != \", \"p_validation_\", metric)\n",
    "    print(day)\n",
    "    df_1 = df[df['day'] <= day]\n",
    "    df_1 = df_1 if rfm is None else df_1[df_1['rfm'] == rfm]\n",
    "    click1 =  sum(list(df_1[df_1['is_control'] == 0][metric]))\n",
    "    click2 =  sum(list(df_1[df_1['is_control'] == 1][metric]))\n",
    "    n1 = len(df_1[df_1['is_control'] == 0])\n",
    "    n2 = len(df_1[df_1['is_control'] == 1])\n",
    "    return click1, click2, n1, n2, list(df_1[df_1['is_control'] == 0][metric]), list(df_1[df_1['is_control'] == 1][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates t value, and confidence intervl of each metric\n",
    "def definition_of_t_test(click1, click2, n1, n2):   \n",
    "    pval, confidence_intervals = calculate_t_test(click1 / n1, click2 / n2, click1, click2, n1, n2)\n",
    "    print(1 - pval, click1 / n1, click2 / n2, \"HO REJECTED!\" if 1 - pval > 0.975 or 1 - pval < 0.25 else \"HO ACCEPTED!\")\n",
    "    return [1 - pval, \"HO REJECTED!\" if 1 - pval > 0.975 or 1 - pval < 0.25 else \"HO ACCEPTED!\", confidence_intervals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian method of function to calculate probability of X data given of parameter\n",
    "def bayesian_approach(c_val, v_val):\n",
    "    \n",
    "    def plot_p_value(x, y1, y2, p1, p2):\n",
    "        plt.plot(x, y1, label=\"control p: %.4f\" % p1)\n",
    "        plt.plot(x, y2, label=\"validation p: %.4f\" % p2)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    x = np.linspace(0, 1, 200)\n",
    "    a_control, b_control = 1, 1\n",
    "    a_val, b_val = 1, 1\n",
    "    number_of_sample = max(len(c_val), len(v_val))\n",
    "    for ind in list(range(max(len(c_val), len(v_val)))):\n",
    "        # control set a, b updating\n",
    "        try:\n",
    "            a_control += c_val[ind] # click\n",
    "            b_control += abs(c_val[ind] - 1) # non-click\n",
    "        except:\n",
    "            if ind + 1 == len(c_val):\n",
    "                print(\"out of index\")\n",
    "        \n",
    "        # validation set a, b updating\n",
    "        try:\n",
    "            a_val += v_val[ind] # click\n",
    "            b_val += abs(v_val[ind] - 1) # non-click\n",
    "        except:\n",
    "            if ind + 1 == len(v_val):\n",
    "                print(\"out of index\")\n",
    "    \n",
    "        if ind in [100, int((number_of_sample *3) / 4)]:\n",
    "            print(\"number of sample :\", ind)\n",
    "            y_control = beta.pdf(x, a_control, b_control)\n",
    "            y_validation = beta.pdf(x, a_val, b_val)\n",
    "            plot_p_value(x, y_control, y_validation, a_control / (a_control + b_control), a_val / (a_val + b_val))\n",
    "            \n",
    "    control_p_values = stats.beta.rvs(a_control, b_control, size=len(c_val))\n",
    "    validation_p_values = stats.beta.rvs(a_val, b_val, size=len(v_val))  \n",
    "    sample_size = min(len(control_p_values), len(validation_p_values))\n",
    "    wins = validation_p_values[:sample_size] > control_p_values[:sample_size]\n",
    "    return  np.mean(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions to merge all outputs into one arrary to finalized\n",
    "def test_control_validation_sets(ab_test_total, metric, day, rfm):\n",
    "    click_control, click_vald, n_control, n_vald, _control, _validation = get_metrics(ab_test_total, metric, day, rfm)\n",
    "    print(click_control, click_vald, n_control, n_vald, day, metric)\n",
    "    t_test_outputs = definition_of_t_test(click_control, click_vald, n_control, n_vald)\n",
    "    print(\"chi squared check :\")\n",
    "    chi_square_test_outputs = chi_square_test(n_control, click_control, n_vald, click_vald)\n",
    "    print(\"Bayesian approach :\")\n",
    "    wins = bayesian_approach(_control, _validation)\n",
    "    print(str(round(wins, 3) * 100), \" % times validation set of CTR is bigger than control set of CTR\")\n",
    "    return click_control / n_control, click_vald / n_vald, np.mean(wins) ,t_test_outputs, chi_square_test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['login_count', 'basket_count', 'order_screen_count', 'order_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you are running with csv file.\n",
    "if is_from_db:\n",
    "    days = list(pd.read_sql(\"\"\"SELECT date FROM designingtest GROUP BY date ORDER BY date ASC\"\"\", connection_abtestdb)['date'])\n",
    "    query = \"\"\"\n",
    "            SELECT\n",
    "                client_id,\n",
    "                is_control,\n",
    "                date as day,\n",
    "                sum(CASE WHEN session IS NOT NULL THEN 1 ELSE 0 END) as session_count,\n",
    "                sum(CASE WHEN login IS NOT NULL THEN 1 ELSE 0 END) as login_count,\n",
    "                sum(CASE WHEN basket IS NOT NULL THEN 1 ELSE 0 END) as basket_count,\n",
    "                sum(CASE WHEN order_screen IS NOT NULL THEN 1 ELSE 0 END) as order_screen_count,\n",
    "                sum(CASE WHEN ordered IS NOT NULL THEN 1 ELSE 0 END) as order_count\n",
    "            \n",
    "            FROM designingtest WHERE date >= '{}' and date <= '{}'\n",
    "            GROUP BY client_id, is_control, date\n",
    "    \"\"\".format(str(min(days))[0:10], str(max(days))[0:10])\n",
    "    ab_test_total = pd.read_sql(query, connection_abtestdb)\n",
    "    ab_test_total = pd.merge(ab_test_total, segments, on='client_id', how='left')\n",
    "    ab_test_total['rfm'] = ab_test_total.apply(lambda row: \n",
    "                                               str(row['recency_segment']) + '_' + \n",
    "                                               str(row['frequency_segment']) + '_' + str(row['monetary_segment']) ,axis=1)\n",
    "    \n",
    "    print(len(ab_test_total))\n",
    "    rfm_segments = list(ab_test_total['rfm'].unique())\n",
    "    ab_test_total.head(1)\n",
    "else:\n",
    "    try:\n",
    "        ab_test_total = pd.read_csv('ab_test_data.csv')\n",
    "        ab_test_total['day'] = ab_test_total['day'].apply(lambda x: datetime.datetime.strptime(str(x)[0:10], '%Y-%m-%d'))\n",
    "        days = list(ab_test_total['day'].unique())\n",
    "        rfm_segments = list(ab_test_total['rfm'].unique())\n",
    "    except:\n",
    "        print(\"please check file has been unziped.\")\n",
    "        print(\"please check ab_test_data.csv is at same directory with .ipynb notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# it will take for a while, approximatelly  7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = df_list_rfm = []\n",
    "p_value_contol = p_value_validation = win_list = []\n",
    "for day in days:\n",
    "    print(\" day :\", str(day)[0:10])\n",
    "    for metric in metrics:        \n",
    "        p_control, p_validation, win_ratio, t_test, chi_square = test_control_validation_sets(ab_test_total, metric, day, None)\n",
    "        df_list.append({'p_control': p_control, \n",
    "                        'p_validation': p_validation, \n",
    "                        'win_ratio': win_ratio, \n",
    "                        't_test_p_value': t_test[0],\n",
    "                        't_test_H0': t_test[1],\n",
    "                        't_test_left_tail': t_test[2][0],\n",
    "                        't_test_right_tail': t_test[2][1],\n",
    "                        'chi_square_p_value': chi_square[0],\n",
    "                        'chi_square_H0': chi_square[1],\n",
    "                        'bayesian_approach_confidence': win_ratio,\n",
    "                        'day': day,\n",
    "                        'metrics': metric\n",
    "                       })\n",
    "        # check for each segment of Control and Validation Samples\n",
    "        for rfm in rfm_segments:\n",
    "            print(\"rfm segment :\", rfm)\n",
    "            try: # days there isn`t any transaction of some segments of clients\n",
    "                p_control, p_validation, win_ratio, t_test, chi_square = test_control_validation_sets(ab_test_total, metric, day, \n",
    "                                                                                                       rfm)\n",
    "                \n",
    "                df_list_rfm.append({'p_control': p_control, \n",
    "                                    'p_validation': p_validation, \n",
    "                                    'win_ratio': win_ratio, \n",
    "                                    't_test_p_value': t_test[0],\n",
    "                                    't_test_H0': t_test[1],\n",
    "                                    't_test_left_tail': t_test[2][0],\n",
    "                                    't_test_right_tail': t_test[2][1],\n",
    "                                    'chi_square_p_value': chi_square[0],\n",
    "                                    'chi_square_H0': chi_square[1],\n",
    "                                    'bayesian_approach_confidence': win_ratio,\n",
    "                                    'rfm': rfm,\n",
    "                                    'day': day,\n",
    "                                    'metrics': metric\n",
    "                                   })\n",
    "            except:\n",
    "                print(\"not enough sample to test!\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_ab_test_results = pd.DataFrame(df_list)\n",
    "daily_ab_test_results_rfm = pd.DataFrame(df_list_rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_ab_test_results.to_csv('daily_ab_test_results.csv')\n",
    "daily_ab_test_results_rfm.to_csv('daily_ab_test_results_rfm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login CTR\n",
    "- Variation Screen has an impact to clients of logins almost %80. O average It is 0.35 and jumps to on average 0.41.\n",
    "- However, we concluded the test with 8241 and it improves only 500 of client on average. \n",
    "- This is a huge increase if we run on real time. For instance, if we have  10 milions vistors in each days, it would be 600k of increase in each days of vistores of logins from 3.5 million to 4.1 milion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basket CTR\n",
    "- Variation Screen has an impact to clients of basket creating almost 60% on average. It is 0.12 and jumps to on average 0.18.\n",
    "- However, we concluded the test with 8241 and it improves only 500 of client on average. \n",
    "- This is a huge increase if we run on real time. For instance, if we have  10 milions visitors in each days, it would be 600k of increase in each days of visitors of adding any item to basket count from 1.2 million to 1.8 milion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payment Screen CTR\n",
    "- Variation Screen has an impact to clients of order screen clicks almost 38 % on average It is 0.05 and jumps to on average 0.08.\n",
    "- However, we concluded the test with 8241 and it improves only 247 of client on average. \n",
    "- This is a huge increase if we run on real time. For instance, if we have 10 milions visitors in each days, it would be 300k of increase in each days of visitores of payment screen click count from 500k to 800k milion.\n",
    "### It seems that when we continue our test enviroment this CTR values will close to each other more. That is why wee need to carry on data collecting for payment screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order CTR\n",
    "- Variation Screen has an impact to clients of basket creating count almost 38% on average It is 0.05 and jumps to on average 0.08.\n",
    "- However, we concluded the test with 8241 and it improves only 247 of client on average. \n",
    "- This is a huge increase if we run on real time. For instance, if we have 10 milions visitors in each days, it would be 300k of increase in each days of visitores of payment screen click count from 500k to 800k milion.\n",
    "### This CTR values are also not well tested too. It must be more data in order to get the actual CTR improvements. Test must be continued for Order count CTR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualization = daily_ab_test_results.pivot_table(index=['day', 'metrics'] , aggfunc={'p_control': 'mean', \n",
    "                                                                     'p_validation': 'mean'}).reset_index()\n",
    "\n",
    "for metric in list(visualization['metrics'].unique()):\n",
    "    print(\"metric :\", metric)\n",
    "    _df = visualization.query(\"metrics == @metric\")\n",
    "    x = [str(i)[0:10] for i in _df['day']]\n",
    "    y = _df['p_control'].values\n",
    "    plt.plot(x, y)\n",
    "    y = _df['p_validation'].values\n",
    "    plt.plot(x, y)\n",
    "    plt.xticks(x, x, rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date = list(visualization.sort_values(by = 'day', ascending=False)['day'])[5]\n",
    "visualization['avg_diff_of_CTR'] = visualization['p_validation'] - visualization['p_control']\n",
    "visualization_1 = visualization.query(\"day >= @date\").pivot_table(index='metrics', \n",
    "                                                                  aggfunc={'avg_diff_of_CTR': 'mean'}).reset_index()\n",
    "visualization_1['client_count'] = 10000000 * visualization_1['avg_diff_of_CTR']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = list(visualization_1.metrics)\n",
    "y_pos = np.arange(len(objects))\n",
    "performance_1 = list(visualization_1.avg_diff_of_CTR)\n",
    "plt.bar(y_pos, performance_1, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('CTR')\n",
    "plt.title('Metrics of CTR Differences From Control to Validation')\n",
    "plt.show()\n",
    "objects = list(visualization_1.metrics)\n",
    "y_pos = np.arange(len(objects))\n",
    "performance_2 = list(visualization_1.client_count)\n",
    "plt.bar(y_pos, performance_2, align='center', alpha=0.5)\n",
    "plt.ylabel('client count difference After New Screen seen')\n",
    "plt.title('What If This Company Has 10 millions of Visitors Daily')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segments of CTR Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualization_2 = daily_ab_test_results_rfm.pivot_table(index=['day', 'metrics', 'rfm'] , aggfunc={'p_control': 'mean', \n",
    "                                                                                                    'p_validation': 'mean'}).reset_index()\n",
    "\n",
    "for rfm in list(visualization_2['rfm'].unique()): \n",
    "    print(\" segment :\", rfm)\n",
    "    for metric in list(visualization_2['metrics'].unique()):\n",
    "        _df = visualization_2.query(\"metrics == @metric and rfm == @rfm\")\n",
    "        x = [str(i)[0:10] for i in _df['day']]\n",
    "        y = _df['p_control'].values\n",
    "        plt.plot(x, y, label= metric.split('_')[0] + ' c.' )\n",
    "        y = _df['p_validation'].values\n",
    "        plt.plot(x, y, label= metric.split('_')[0] + ' v.' )\n",
    "        plt.grid(True)\n",
    "        plt.xticks(x, x, rotation='vertical')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B Test Segment Explation:\n",
    "- Segmentaion is created with the popular markettn client segmentation which is RFM (Recency - Frequency- Monetary)\n",
    "- Recency: How frequently client is ordered.\n",
    "- Frequency: How frequently client visits.\n",
    "- Monetary: represents average of basket values of clients.\n",
    "- Each metrics segmented into 4 groups. There are at most 4 *4 * = 64 group can be created. 1; represents the best, 4; the worst segment.\n",
    "- Some segments need more data as like 3_1_3.\n",
    "- 1_1_3, 2_2_2, 3_2_1, 3_2_2, 3_3_1, 3_3_2, 3_4_1 segmets are well has better impact pn validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_segments = ['1_1_3', '2_2_2', '3_2_1', '3_2_2', '3_3_1', '3_3_2', '3_4_1']\n",
    "fequencies = pd.DataFrame([int(rfm.split('_')[0]) for rfm in best_segments]).rename(columns={0: 'frequency'})\n",
    "fequencies['freq_count'] = fequencies['frequency']\n",
    "fequencies = fequencies.pivot_table(index='frequency', aggfunc={'freq_count': 'count'}).reset_index()\n",
    "recencies = pd.DataFrame([int(rfm.split('_')[1]) for rfm in best_segments]).rename(columns={0: 'recency'})\n",
    "recencies['r_count'] = recencies['recency']\n",
    "recencies = recencies.pivot_table(index='recency', aggfunc={'r_count': 'count'}).reset_index()\n",
    "monetaries = pd.DataFrame([int(rfm.split('_')[2]) for rfm in best_segments]).rename(columns={0: 'monetary'})\n",
    "monetaries['m_count'] = monetaries['monetary']\n",
    "monetaries = monetaries.pivot_table(index='monetary', aggfunc={'m_count': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fequencies, recencies, monetaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of segmented clients:\n",
    "- The client who pays much loves the new screen :)\n",
    "- However, these clients are not frequently vistors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Next Analysis\n",
    "- This segmented clients of attributtes must be investiged. \n",
    "- Are their frequency segments are changing?  \n",
    "- What is their order period has change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more information pls check gitHub reporsitory:\n",
    "https://github.com/caglanakpinar/a_b_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard to Visualization Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I use dash library to visualize outputs.\n",
    "- Make sure you install dash library. Both conda install and pip install coments are worked on that.\n",
    "- In addition to Dash library, you need to install plotly too.\n",
    "- In order to use plotly library you need license. However, they allow us to use it locally offline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check these libraries can be imported successfully\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the code .ipynb notebook below.\n",
    "- Make sure is at same directory as current file.\n",
    "- Make sure 'daily_ab_test_results.csv' and 'daily_ab_test_results_rfm.csv' are in the same directory.\n",
    "- If there aren`t any of these .csv file, you can use defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ./visualization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
